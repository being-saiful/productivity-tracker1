# -*- coding: utf-8 -*-
"""proT_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RGxsryTq1Un0w3rVEWflwe0bDQsu6mvR
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# Create project folder
# %mkdir -p /content/drive/MyDrive/productivity_tracker
# %cd /content/drive/MyDrive/productivity_tracker

# Install dependencies
# %pip install transformers datasets torch scikit-learn pandas peft bitsandbytes accelerate
# %pip install fastapi uvicorn sqlalchemy youtube-transcript-api  # Added YouTube API

# Restart runtime after installs
print("Restart the runtime (Runtime > Restart runtime) before proceeding!")

import pandas as pd
import json

# Create app classification dataset (excluding YouTube for now)
app_data = [
    {"app_name": "VS Code", "category": "Development", "career": "software engineer", "is_productive": 1},
    {"app_name": "GitHub", "category": "Development", "career": "software engineer", "is_productive": 1},
    {"app_name": "TikTok", "category": "Social", "career": "software engineer", "is_productive": 0},
    {"app_name": "Scrivener", "category": "Writing", "career": "writer", "is_productive": 1},
    {"app_name": "Grammarly", "category": "Writing", "career": "writer", "is_productive": 1},
    {"app_name": "Photoshop", "category": "Design", "career": "graphic designer", "is_productive": 1},
    {"app_name": "Figma", "category": "Design", "career": "graphic designer", "is_productive": 1},
    {"app_name": "Instagram", "category": "Social", "career": "social media manager", "is_productive": 0},
    {"app_name": "Netflix", "category": "Entertainment", "career": "general", "is_productive": 0},
    {"app_name": "Pinterest", "category": "Social", "career": "graphic designer", "is_productive": 0},
    {"app_name": "Facebook", "category": "Social", "career": "general", "is_productive": 0},
    {"app_name": "Candy Crush", "category": "Gaming", "career": "general", "is_productive": 0}
]

df_apps = pd.DataFrame(app_data)
df_apps.to_csv("apps_career_productivity.csv", index=False)
print("App dataset saved:", df_apps.shape)

# Create YouTube content classification dataset
youtube_data = [
    # Software Engineer content
    {"title": "Python Django Tutorial for Beginners", "description": "Learn web development with Django", "channel": "Programming Channel", "career": "software engineer", "category": "Learning", "is_productive": 1},
    {"title": "JavaScript Crash Course", "description": "Master JavaScript in 1 hour", "channel": "Web Dev Simplified", "career": "software engineer", "category": "Learning", "is_productive": 1},
    {"title": "Gaming Video - Fortnite Highlights", "description": "Best Fortnite gameplay moments", "channel": "Gaming Channel", "career": "software engineer", "category": "Entertainment", "is_productive": 0},
    {"title": "Minecraft Lets Play", "description": "Playing Minecraft survival mode", "channel": "Gaming Channel", "career": "software engineer", "category": "Entertainment", "is_productive": 0},

    # Writer content
    {"title": "Creative Writing Tips", "description": "How to write better stories", "channel": "Writing Tips", "career": "writer", "category": "Writing", "is_productive": 1},
    {"title": "Grammar Rules Explained", "description": "English grammar tutorial", "channel": "Education Channel", "career": "writer", "category": "Writing", "is_productive": 1},
    {"title": "Comedy Skit Compilation", "description": "Funny videos compilation", "channel": "Comedy Channel", "career": "writer", "category": "Entertainment", "is_productive": 0},

    # General productive content
    {"title": "Time Management Techniques", "description": "Productivity tips for everyone", "channel": "Productivity Master", "career": "general", "category": "Learning", "is_productive": 1},
    {"title": "Music Video - Pop Song", "description": "Latest music hit", "channel": "Music Channel", "career": "general", "category": "Entertainment", "is_productive": 0},

    # More examples to expand dataset
    {"title": "Machine Learning Fundamentals", "description": "Introduction to ML algorithms", "channel": "AI Channel", "career": "software engineer", "category": "Learning", "is_productive": 1},
    {"title": "React Hooks Tutorial", "description": "Learn React hooks in depth", "channel": "React Mastery", "career": "software engineer", "category": "Learning", "is_productive": 1},
    {"title": "Cooking Show - Pasta Recipe", "description": "How to make Italian pasta", "channel": "Cooking Channel", "career": "general", "category": "Entertainment", "is_productive": 0},
]

df_youtube = pd.DataFrame(youtube_data)
df_youtube.to_csv("youtube_content_classification.csv", index=False)
print("YouTube dataset saved:", df_youtube.shape)

# Create roadmap generation dataset
roadmap_data = [
    {
        "prompt": "Generate a 12-month roadmap for a beginner software engineer",
        "completion": """
Month 1-2: Learn Python fundamentals, data structures, and algorithms
Month 3-4: Build 3 small projects (CLI app, web scraper, automation script)
Month 5-6: Study Git, GitHub, and basic web development
Month 7-8: Learn a framework (Django/React) and build a full-stack project
Month 9-10: Understand databases, APIs, and deployment
Month 11: Contribute to open source and prepare resume
Month 12: Apply for junior developer positions
        """
    },
    {
        "prompt": "Generate a roadmap for an aspiring writer",
        "completion": """
Month 1-2: Daily writing practice (500 words), read 10 books in your genre
Month 3-4: Start a blog, learn storytelling fundamentals
Month 5-6: Complete first short story, join writing groups
Month 7-8: Learn publishing industry, build author platform
Month 9-10: Write first novel draft, get feedback
Month 11: Revise manuscript, research agents
Month 12: Submit to publishers or self-publish
        """
    },
    {
        "prompt": "Generate a roadmap for a data scientist",
        "completion": """
Month 1-2: Master Python, pandas, numpy, and basic statistics
Month 3-4: Learn data visualization (matplotlib, seaborn, Tableau)
Month 5-6: Study machine learning algorithms and scikit-learn
Month 7-8: Work on 3 Kaggle projects and build portfolio
Month 9-10: Learn SQL, databases, and big data tools
Month 11: Study deep learning (PyTorch/TensorFlow)
Month 12: Prepare for interviews and apply for jobs
        """
    }
]

with open("roadmap_finetune.jsonl", "w") as f:
    for item in roadmap_data:
        f.write(json.dumps(item) + "\n")
print("Roadmap dataset saved:", len(roadmap_data), "examples")

# === FIX: Add ALL required imports ===
import torch
import pandas as pd
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
from sklearn.model_selection import train_test_split
import os

# === FIX: Load and verify data ===
print("Loading dataset...")
try:
    df = pd.read_csv("apps_career_productivity.csv")
    print(f"Data loaded: {df.shape[0]} rows")
except FileNotFoundError:
    print("apps_career_productivity.csv not found. Run Cell 2 first!")
    raise

# === FIX: Verify columns exist ===
required_columns = ["app_name", "category", "career", "is_productive"]
for col in required_columns:
    if col not in df.columns:
        print(f"Missing column: {col}")
        raise ValueError(f"Dataset missing column: {col}")

# === FIX: Create text column safely ===
df["text"] = df["app_name"].astype(str) + " [SEP] " + df["category"].astype(str) + " [SEP] " + df["career"].astype(str)

# === FIX: Convert label to standard name ===
df["labels"] = df["is_productive"].astype(int)  # Trainer expects "labels" column
print(f"Sample data:\n{df[['text', 'labels']].head()}")

# === FIX: Split with stratification for balanced classes ===
train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["labels"]  # Keeps class balance
)

print(f"Train: {len(train_df)} | Val: {len(val_df)}")
print(f"Train class distribution:\n{train_df['labels'].value_counts()}")

# === FIX: Convert to Dataset correctly ===
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))

# === FIX: Load tokenizer with error handling ===
print("Loading tokenizer...")
try:
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    print("Tokenizer loaded")
except Exception as e:
    print(f"Tokenizer error: {e}")
    raise

# === FIX: Tokenization function with explicit return ===
def tokenize_function(examples):
    # Tokenize texts
    tokenized = tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=64
    )
    # Add labels as torch tensors
    tokenized["labels"] = examples["labels"]
    return tokenized

# === FIX: Apply tokenization with batching ===
print("Tokenizing datasets...")
train_dataset = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text", "is_productive"]
)
val_dataset = val_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text", "is_productive"]
)

# === FIX: Set format for PyTorch tensors ===
train_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)
val_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

print("Tokenization complete")

# === FIX: Check GPU and reduce memory usage ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

if torch.cuda.is_available():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"GPU Memory: {gpu_memory:.2f} GB")

    # Adjust batch size based on GPU memory
    batch_size = 16 if gpu_memory > 8 else 8
else:
    batch_size = 4
    print("Running on CPU - training will be slow!")

# === FIX: Initialize model with gradient checkpointing for memory efficiency ===
print("Loading BERT model...")
try:
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-uncased",
        num_labels=2,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )

    # Enable gradient checkpointing to save memory
    model.gradient_checkpointing_enable()

    model.to(device)
    print("Model loaded")
except Exception as e:
    print(f"Model loading error: {e}")
    raise

# === FIX: Optimized TrainingArguments for Colab ===
training_args = TrainingArguments(
    output_dir="./app_classifier",
    eval_strategy="steps",  # Faster feedback
    eval_steps=10,  # Evaluate every 10 steps
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size * 2,  # Double for eval
    num_train_epochs=3,  # Reduced from 5 to prevent timeout
    weight_decay=0.01,
    save_strategy="steps",
    save_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    report_to=["none"],  # Disable wandb/comet
    dataloader_pin_memory=False,  # Prevents Colab memory issues
    # Add these to prevent freezing:
    dataloader_drop_last=True,
    logging_steps=5,
)

# === FIX: Add compute_metrics function ===
def compute_metrics(eval_pred):
    """Calculate accuracy for evaluation"""
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

# === FIX: Initialize Trainer with all required parameters ===
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,  # IMPORTANT: For proper padding
    compute_metrics=compute_metrics,
)

# === FIX: Train with error catching ===
print("Starting training...")
try:
    trainer.train()
    print("Training completed successfully!")
except Exception as e:
    print(f"Training failed: {e}")
    print("Try reducing batch_size or num_train_epochs")
    raise

# === FIX: Save models with verification ===
print("Saving models...")
model.save_pretrained("./app_classifier_model")
tokenizer.save_pretrained("./app_classifier_model")

# Verify save
if os.path.exists("./app_classifier_model/config.json"):
    print("App classification model saved to Google Drive!")
else:
    print("Save failed - check permissions")

# YouTube Content Classifier Training
import torch
import pandas as pd
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
from sklearn.model_selection import train_test_split
import os

# === Load and prepare YouTube dataset ===
print("Loading YouTube dataset...")
try:
    df_yt = pd.read_csv("youtube_content_classification.csv")
    print(f"YouTube data loaded: {df_yt.shape[0]} rows")
except FileNotFoundError:
    print("youtube_content_classification.csv not found. Run Cell 2 first!")
    raise

# === Create text column for YouTube content analysis ===
df_yt["text"] = (
    df_yt["title"].astype(str) + " [SEP] " +
    df_yt["description"].astype(str) + " [SEP] " +
    df_yt["channel"].astype(str) + " [SEP] " +
    df_yt["career"].astype(str)
)

# === Convert label to standard name ===
df_yt["labels"] = df_yt["is_productive"].astype(int)
print(f"Sample YouTube data:\n{df_yt[['text', 'labels']].head()}")

# === Split dataset ===
train_df_yt, val_df_yt = train_test_split(
    df_yt,
    test_size=0.2,
    random_state=42,
    stratify=df_yt["labels"]
)

print(f"Train: {len(train_df_yt)} | Val: {len(val_df_yt)}")

# === Convert to Dataset ===
train_dataset_yt = Dataset.from_pandas(train_df_yt.reset_index(drop=True))
val_dataset_yt = Dataset.from_pandas(val_df_yt.reset_index(drop=True))

# === Load tokenizer ===
print("Loading tokenizer for YouTube classifier...")
tokenizer_yt = BertTokenizer.from_pretrained("bert-base-uncased")

# === Tokenization function ===
def tokenize_function_yt(examples):
    tokenized = tokenizer_yt(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128  # Longer for YouTube content
    )
    tokenized["labels"] = examples["labels"]
    return tokenized

# === Apply tokenization ===
print("Tokenizing YouTube datasets...")
train_dataset_yt = train_dataset_yt.map(
    tokenize_function_yt,
    batched=True,
    remove_columns=["text", "is_productive", "title", "description", "channel", "career", "category"]
)
val_dataset_yt = val_dataset_yt.map(
    tokenize_function_yt,
    batched=True,
    remove_columns=["text", "is_productive", "title", "description", "channel", "career", "category"]
)

# === Set format for PyTorch tensors ===
train_dataset_yt.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)
val_dataset_yt.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

print("YouTube tokenization complete")

# === Setup device and batch size ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

batch_size = 8 if torch.cuda.is_available() else 4

# === Initialize YouTube model ===
print("Loading BERT model for YouTube classifier...")
model_yt = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

if torch.cuda.is_available():
    model_yt.gradient_checkpointing_enable()
    model_yt.to(device)

print("YouTube model loaded")

# === Training arguments ===
training_args_yt = TrainingArguments(
    output_dir="./youtube_classifier",
    eval_strategy="epoch", # Evaluation strategy changed to epoch
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=1, # Reduced eval batch size for small validation set
    num_train_epochs=5,
    weight_decay=0.01,
    save_strategy="epoch", # Changed to epoch to match eval_strategy
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    greater_is_better=True,
    report_to=["none"],
    dataloader_pin_memory=False,
    dataloader_drop_last=True,
    logging_steps=3,
)

# === Compute metrics function ===
def compute_metrics_yt(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

# === Initialize Trainer ===
trainer_yt = Trainer(
    model=model_yt,
    args=training_args_yt,
    train_dataset=train_dataset_yt,
    eval_dataset=val_dataset_yt,
    tokenizer=tokenizer_yt,
    compute_metrics=compute_metrics_yt,
)

# === Train model ===
print("Starting YouTube classifier training...")
try:
    trainer_yt.train()
    print("YouTube classifier training completed successfully!")
except Exception as e:
    print(f"YouTube classifier training failed: {e}")
    raise

# === Save models ===
print("Saving YouTube classifier models...")
model_yt.save_pretrained("./youtube_classifier_model")
tokenizer_yt.save_pretrained("./youtube_classifier_model")

print("YouTube classifier model saved!")

# Use a smaller, Colab-friendly model without 4-bit quantization
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset

# Load dataset
dataset = load_dataset("json", data_files="roadmap_finetune.jsonl")
print("Dataset loaded:", dataset)

# Use a smaller model (DistilGPT2 works reliably in Colab)
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Load model on GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# Print model size
print(f"Model loaded: {model_name} on {device}")
print(f"Trainable parameters: {sum(p.numel() for p in model.parameters()):,}")

# Tokenize function
def tokenize_function(examples):
    prompt = examples["prompt"]
    completion = examples["completion"]
    # Format: prompt -> completion
    text = f"### Prompt: {prompt}\n### Roadmap: {completion}"
    return tokenizer(
        text,
        truncation=True,
        max_length=512,
        padding="max_length",
        return_tensors="pt"
    )

# Apply tokenization
tokenized_dataset = dataset.map(tokenize_function, remove_columns=["prompt", "completion"])
print("Tokenization complete")

# Training arguments (optimized for Colab Free)
training_args = TrainingArguments(
    output_dir="./roadmap_generator",
    num_train_epochs=3,  # Reduce if Colab times out
    per_device_train_batch_size=2,  # Small batch for memory
    gradient_accumulation_steps=8,  # Effective batch size = 16
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_steps=5,
    save_strategy="epoch",
    fp16=torch.cuda.is_available(),  # Mixed precision only if GPU
    dataloader_pin_memory=False,  # Prevents memory issues
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

# Train with progress bar
print("Starting training...")
trainer.train()
print("Training complete!")

# Save models
model.save_pretrained("./roadmap_generator_model")
tokenizer.save_pretrained("./roadmap_generator_model")
print("Models saved to Google Drive!")

"""### Generate a Roadmap
Now that the roadmap generator model is trained, you can use it to generate roadmaps based on your custom prompts.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the saved tokenizer and model
model_path = "./roadmap_generator_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# Set the device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def generate_roadmap(prompt_text, max_length=512, num_return_sequences=1):
    input_text = f"### Prompt: {prompt_text}\n### Roadmap:"
    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length).to(device)

    # Generate output
    outputs = model.generate(
        **inputs,
        max_length=max_length + 100, # Allow some room for completion
        num_return_sequences=num_return_sequences,
        do_sample=True, # Use sampling for more creative output
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        repetition_penalty=1.2, # Discourage repetition
        pad_token_id=tokenizer.eos_token_id
    )

    generated_texts = []
    for i, output in enumerate(outputs):
        text = tokenizer.decode(output, skip_special_tokens=True)
        # Remove the input prompt to only show the generated roadmap
        generated_roadmap = text.split("### Roadmap:", 1)[-1].strip()
        generated_texts.append(f"\n--- Generated Roadmap {i+1} ---\n{generated_roadmap}")
    return "\n".join(generated_texts)

# Example Usage:
user_prompt = "Generate a 12-month roadmap for an aspiring game developer"
roadmap = generate_roadmap(user_prompt)
print(roadmap)

# === FIX 1: Check if models exist first ===
import os

if not os.path.exists("./app_classifier_model"):
    print("Error: App classifier model not found! Run Cell 3 first.")
else:
    print("App classifier model found")

if not os.path.exists("./youtube_classifier_model"):
    print("Error: YouTube classifier model not found! Run Cell 4 first.")
else:
    print("YouTube classifier model found")

if not os.path.exists("./roadmap_generator_model"):
    print("Error: Roadmap generator model not found! Run Cell 5 first.")
else:
    print("Roadmap generator model found")

# === TEST APP CLASSIFIER ===
from transformers import pipeline

def test_app_classifier(app_name, category, career):
    """Test the classification model"""
    try:
        classifier = pipeline(
            "text-classification",
            model="./app_classifier_model",
            tokenizer="./app_classifier_model"
        )

        text = f"{app_name} [SEP] {category} [SEP] {career}"
        result = classifier(text)

        return {
            "app": app_name,
            "career": career,
            "is_productive": result[0]["label"] == "LABEL_1",
            "confidence": f"{result[0]['score']:.2%}"
        }
    except Exception as e:
        return f"Error: {str(e)}"

# Test cases
print("="*50)
print("Testing App Classifier:")
print("="*50)
print(test_app_classifier("VS Code", "Development", "software engineer"))
print(test_app_classifier("TikTok", "Social", "software engineer"))
print(test_app_classifier("Final Cut Pro", "Video Editing", "content creator"))

# === TEST YOUTUBE CLASSIFIER ===
def test_youtube_classifier(video_title, video_description, channel, career):
    """Test the YouTube content classification model"""
    try:
        classifier = pipeline(
            "text-classification",
            model="./youtube_classifier_model",
            tokenizer="./youtube_classifier_model"
        )

        text = f"{video_title} [SEP] {video_description} [SEP] {channel} [SEP] {career}"
        result = classifier(text)

        # Map categories based on career context
        career_categories = {
            "software engineer": {"LABEL_1": "Learning", "LABEL_0": "Entertainment"},
            "writer": {"LABEL_1": "Writing", "LABEL_0": "Entertainment"},
            "general": {"LABEL_1": "Learning", "LABEL_0": "Entertainment"}
        }

        category_map = career_categories.get(career, {"LABEL_1": "Productive", "LABEL_0": "Unproductive"})
        category = category_map.get(result[0]["label"], "Unknown")

        return {
            "video": video_title,
            "career": career,
            "category": category,
            "is_productive": result[0]["label"] == "LABEL_1",
            "confidence": f"{result[0]['score']:.2%}"
        }
    except Exception as e:
        return f"Error: {str(e)}"

# Test YouTube classifier
print("\n" + "="*50)
print("Testing YouTube Classifier:")
print("="*50)
print(test_youtube_classifier("Python Django Tutorial", "Learn web development", "Programming Channel", "software engineer"))
print(test_youtube_classifier("Gaming Video", "Fortnite gameplay", "Gaming Channel", "software engineer"))
print(test_youtube_classifier("Creative Writing Tips", "Writing techniques", "Writing Tips", "writer"))

# === TEST ROADMAP GENERATOR ===
def test_roadmap_generator(career_path):
    """Test the roadmap generation"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM

        tokenizer = AutoTokenizer.from_pretrained("./roadmap_generator_model")
        model = AutoModelForCausalLM.from_pretrained("./roadmap_generator_model")

        # Set pad token if not set
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        prompt = f"Generate a 12-month roadmap for a beginner {career_path}"
        input_text = f"### Prompt: {prompt}\n### Roadmap:"

        inputs = tokenizer(input_text, return_tensors="pt", padding=True)

        # Generate with parameters that prevent repetition
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id
        )

        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Extract only the roadmap part
        roadmap = full_text.split("### Roadmap:")[1].split("###")[0].strip()

        return f"Roadmap for '{career_path}':\n\n{roadmap}"

    except Exception as e:
        return f"Error: {str(e)}"

print("\n" + "="*50)
print("Testing Roadmap Generator:")
print("="*50)
print(test_roadmap_generator("mobile app developer"))