import os
from typing import Optional
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

# -------------------------------------------------------------------------
# 1️⃣  Paths where the three model folders live (generated by your training script)
# -------------------------------------------------------------------------
MODEL_DIR_APP = "./app_classifier_model"
MODEL_DIR_YT  = "./youtube_classifier_model"
MODEL_DIR_RM  = "./roadmap_generator_model"

# -------------------------------------------------------------------------
# 2️⃣  Lazy model loading – the first request will spin up the model,
#     after that we reuse the same instance (no extra latency per request)
# -------------------------------------------------------------------------
_app_classifier = None
_yt_classifier   = None
_rm_tokenizer    = None
_rm_model        = None
_rm_device       = None


def load_app_classifier():
    if not os.path.isdir(MODEL_DIR_APP):
        raise RuntimeError("App classifier model not found")
    return pipeline(
        "text-classification",
        model=MODEL_DIR_APP,
        tokenizer=MODEL_DIR_APP,
        return_all_scores=False,
    )


def load_yt_classifier():
    if not os.path.isdir(MODEL_DIR_YT):
        raise RuntimeError("YouTube classifier model not found")
    return pipeline(
        "text-classification",
        model=MODEL_DIR_YT,
        tokenizer=MODEL_DIR_YT,
        return_all_scores=False,
    )


def load_roadmap_generator():
    if not os.path.isdir(MODEL_DIR_RM):
        raise RuntimeError("Roadmap generator model not found")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR_RM)
    model = AutoModelForCausalLM.from_pretrained(MODEL_DIR_RM)

    # some small models (e.g. GPT-2-distil) miss a pad token – give it one
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    return tokenizer, model, device


def get_app_classifier():
    global _app_classifier
    if _app_classifier is None:
        _app_classifier = load_app_classifier()
    return _app_classifier


def get_yt_classifier():
    global _yt_classifier
    if _yt_classifier is None:
        _yt_classifier = load_yt_classifier()
    return _yt_classifier


def get_roadmap_components():
    global _rm_tokenizer, _rm_model, _rm_device
    if _rm_tokenizer is None:
        _rm_tokenizer, _rm_model, _rm_device = load_roadmap_generator()
    return _rm_tokenizer, _rm_model, _rm_device


# -------------------------------------------------------------------------
# 3️⃣  Request bodies
# -------------------------------------------------------------------------
class AppInput(BaseModel):
    app_name: str
    category: str
    career: str


class YoutubeInput(BaseModel):
    title: str
    description: str
    channel: str
    career: str


class RoadmapInput(BaseModel):
    career: str
    level: Optional[str] = None    # optional – you can pass "beginner", "intermediate", …
    prompt: Optional[str] = None   # custom prompt, otherwise a default one is built


# -------------------------------------------------------------------------
# 4️⃣  Health check
# -------------------------------------------------------------------------
@app.get("/health")
def health_check():
    return {"status": "ok", "message": "ML Service is running"}


# -------------------------------------------------------------------------
# 5️⃣  End-points
# -------------------------------------------------------------------------
@app.post("/classify/app")
def classify_app(payload: AppInput):
    try:
        classifier = get_app_classifier()
        text = f"{payload.app_name} [SEP] {payload.category} [SEP] {payload.career}"
        result = classifier(text)[0]          # e.g. {"label":"LABEL_1","score":0.94}
        is_productive = result["label"] == "LABEL_1"
        return {
            "is_productive": is_productive,
            "confidence": float(result["score"]),
            "label": result["label"],
        }
    except Exception as e:
        return {
            "error": str(e),
            "is_productive": None,
            "confidence": 0.0
        }


@app.post("/classify/youtube")
def classify_youtube(payload: YoutubeInput):
    try:
        classifier = get_yt_classifier()
        text = f"{payload.title} [SEP] {payload.description} [SEP] {payload.channel} [SEP] {payload.career}"
        result = classifier(text)[0]
        is_productive = result["label"] == "LABEL_1"
        return {
            "is_productive": is_productive,
            "confidence": float(result["score"]),
            "label": result["label"],
        }
    except Exception as e:
        return {
            "error": str(e),
            "is_productive": None,
            "confidence": 0.0
        }


@app.post("/generate/roadmap")
def generate_roadmap(payload: RoadmapInput):
    try:
        tokenizer, model, device = get_roadmap_components()

        # Build a sensible prompt if the client didn't supply one
        if not payload.prompt:
            base = f"Generate a 12-month roadmap for a {payload.career}"
            if payload.level:
                base += f" at {payload.level} level"
            payload.prompt = base

        input_text = f"### Prompt: {payload.prompt}\n### Roadmap:"
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding="max_length",
        ).to(device)

        # Generation parameters – tuned for a short but creative output
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id,
        )
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract the roadmap part
        if "### Roadmap:" in full_text:
            roadmap = full_text.split("### Roadmap:", 1)[1].strip()
            roadmap = roadmap.split("###")[0].strip()   # cut any trailing sections
        else:
            roadmap = full_text

        steps = [s.strip() for s in roadmap.split("\n") if s.strip()]
        return {"steps": steps}
    except Exception as e:
        return {
            "error": str(e),
            "steps": []
        }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
